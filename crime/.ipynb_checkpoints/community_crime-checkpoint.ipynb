{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "679734e6-c0c7-4d2b-b6e7-434292474624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b0a0efec-9b66-4ac3-abe1-ea3e591c4c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "percents = [\"racepctblack\", \"racePctWhite\", \"racePctAsian\", \"racePctHisp\", \"agePct12t21\", \"agePct12t29\", \"agePct16t24\", \"agePct65up\", \n",
    "     \"pctUrban\", \"pctWWage\", \"pctWFarmSelf\", \"pctWInvInc\", \"pctWSocSec\", \"pctWPubAsst\", \"pctWRetire\", \"PctPopUnderPov\", \"PctLess9thGrade\",\n",
    "     \"PctNotHSGrad\", \"PctBSorMore\", \"PctUnemployed\", \"PctEmploy\", \"PctEmplManu\", \"PctEmplProfServ\", \"PctOccupManu\", \"PctOccupMgmtProf\", \n",
    "     \"MalePctDivorce\", \"MalePctNevMarr\", \"FemalePctDiv\", \"TotalPctDiv\", \"PctFam2Par\", \"PctKids2Par\", \"PctYoungKids2Par\", \"PctTeen2Par\",\n",
    "     \"PctWorkMomYoungKids\", \"PctWorkMom\", \"PctKidsBornNeverMar\", \"PctImmigRecent\", \"PctImmigRec5\", \"PctImmigRec8\", \"PctImmigRec10\", \n",
    "     \"PctRecentImmig\", \"PctRecImmig5\", \"PctRecImmig8\", \"PctRecImmig10\", \"PctSpeakEnglOnly\", \"PctNotSpeakEnglWell\", \"PctLargHouseFam\", \n",
    "     \"PctLargHouseOccup\", \"PctPersOwnOccup\", \"PctPersDenseHous\", \"PctHousLess3BR\", \"PctHousOccup\", \"PctHousOwnOcc\", \"PctVacantBoarded\", \n",
    "     \"PctVacMore6Mos\", \"PctHousNoPhone\", \"PctWOFullPlumb\", \"PctForeignBorn\", \"PctBornSameState\", \"PctSameHouse85\", \"PctSameCity85\",\n",
    "     \"PctSameState85\", \"PolicPerPop\", \"PctPolicWhite\", \"PctPolicBlack\", \"PctPolicHisp\", \"PctPolicAsian\", \"PctPolicMinor\", \n",
    "     \"PctUsePubTrans\", \"LemasPctPolicOnPatr\", \"LemasPctOfficDrugUn\", \"PctBornSameState\", \"PctSameHouse85\", \"PctSameCity85\", \"PctSameState85\"]\n",
    "def convertToPercents(myData):\n",
    "    for column in percents:\n",
    "        if column in myData:\n",
    "            myData[column] = myData[column] / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2291523d-6dbd-4119-b269-2377a56fc0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Cell opens the data from the CSV as rawdata and drops unneccessary and largley missing features from the data\n",
    "rawdata = pd.read_csv(\"crimedata.csv\")\n",
    "# No nebraska or rhode island states\n",
    "\n",
    "# drops all unnecessary columns for analysis(countyCode, etc.)or redudant variables\n",
    "rawdata = rawdata.drop(['countyCode', 'numbUrban', 'communityCode', 'communityName', 'state', 'LandArea','burglaries',\n",
    "                  'larcenies','autoTheft','murders', 'rapes', 'robberies', 'assaults', 'arsons'], axis = 1)\n",
    "\n",
    "#drops columns with lots of missing data || mostly police data\n",
    "rawdata = rawdata.drop([\"LemasSwornFT\", \"LemasSwFTPerPop\", \"LemasSwFTFieldOps\", \"LemasSwFTFieldPerPop\", \"LemasTotalReq\",\n",
    "                  \"LemasTotReqPerPop\", \"PolicReqPerOffic\", \"PolicPerPop\", \"RacialMatchCommPol\", \"PctPolicWhite\",\n",
    "                  \"PctPolicBlack\", \"PctPolicHisp\", \"PctPolicAsian\", \"PctPolicMinor\", \"OfficAssgnDrugUnits\",\n",
    "                  \"NumKindsDrugsSeiz\", \"PolicAveOTWorked\", \"PolicCars\", \"PolicOperBudg\", \"LemasPctPolicOnPatr\",\n",
    "                  \"LemasGangUnitDeploy\", \"PolicBudgPerPop\",'LemasPctOfficDrugUn'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9b016e8c-7176-4efe-8acc-898d6dbd40d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total Features in data: 63\n",
      "PCA(n_components=15)\n",
      "XGBoost MSE:  0.3865062313749055\n",
      "Random Forest MSE:  0.357310455381375\n",
      "Lasso Cross-validated MSE: 0.4102397775103538\n",
      "Lasso R^2 Score: 0.5883069059792174\n",
      "Regression MSE: 0.6336775784156268\n"
     ]
    }
   ],
   "source": [
    "data = rawdata\n",
    "\n",
    "# droppint crime columns and leaving violentCrimePerPop and nonViolentCrimePerPop\n",
    "data = data.drop(['burglPerPop','murdPerPop' , 'robbbPerPop', 'assaultPerPop', 'rapesPerPop','larcPerPop','autoTheftPerPop','arsonsPerPop'], axis = 1)\n",
    "data = data.drop('nonViolPerPop', axis = 1)\n",
    "\n",
    "# Converts all the percentage data to decimals\n",
    "convertToPercents(data)\n",
    "\n",
    "# Drops rows with na values (around 300)\n",
    "data = data.dropna()\n",
    "\n",
    "# Scales Data\n",
    "data_scaled = scaleData(data)\n",
    "data = data_scaled\n",
    "\n",
    "predictorVariable = 'ViolentCrimesPerPop' # Variable to predict in Regression\n",
    "\n",
    "toDrop = [predictorVariable, 'ViolentCrimesPerPop']\n",
    "#toDrop += ['ViolentCrimesPerPop','PctKids2Par','FemalePctDiv','MalePctDivorce','PctKidsBornNeverMar', \n",
    "#          'NumKidsBornNeverMar','MalePctNevMarr', 'PctFam2Par', 'PctYoungKids2Par', 'PctTeen2Par','TotalPctDiv']\n",
    "\n",
    "toDrop += dropCorr(data)\n",
    "y = data[predictorVariable]\n",
    "x = data.drop(toDrop , axis = 1)\n",
    "\n",
    "feature_names = x.columns\n",
    "print(f\"Number of total Features in data: {len(feature_names)}\")\n",
    "\n",
    "pca = runPCA(x,15)\n",
    "print(pca)\n",
    "\n",
    "pca_comps = pca.components_\n",
    "df_loadings = pd.DataFrame(pca_comps, columns = feature_names)\n",
    "\n",
    "#print(df_loadings)\n",
    "\n",
    "\n",
    "\n",
    "regr = runRegression(x, y)\n",
    "#regr_df = pd.DataFrame({\"Features\":feature_names, \"Coefficients\":regr.coef_})\n",
    "#print(regr_df.sort_values(by = \"Coefficients\"))\n",
    "\n",
    "xgbModel = runXGBModel(x, y, False, False)\n",
    "\n",
    "rfModel = runRF(x,y, n_estimators = 50, featRank = False, plot = False)\n",
    "\n",
    "lasso = runLassoModel(x, y)\n",
    "coefs = lasso.coef_\n",
    "\n",
    "# converts coefs percents back to whole numbers to better interpret the percentages\n",
    "if predictorVariable in percents:  \n",
    "    coefs = coefs * 100\n",
    "\n",
    "feature_coef = pd.DataFrame({\"features\": feature_names,\"coefs\": coefs}) # Df of all features and respective coefficeints \n",
    "feature_coef = feature_coef.sort_values(by = \"coefs\") # Sorts feature_coef\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', None) # sets max print rows to None\n",
    "#print(feature_coef)\n",
    "pd.set_option('display.max_rows', 60) # Sets max print rows back to default of 60\n",
    "\n",
    "good_features = feature_coef[feature_coef[\"coefs\"] != 0]\n",
    "#print(good_features)\n",
    "\n",
    "print(f\"Regression MSE: {runRegression(x,y)[4]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "56162088-6ca8-4504-bfdf-8c04b51ef162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"pd.set_option('display.max_rows', None)\\nprint(none_data)\\npd.set_option('display.max_rows', 60)\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_mat = data_scaled.corr()\n",
    "none_data = data.isna().sum()\n",
    "threshold = 0.8\n",
    "\n",
    "# Get the absolute correlation values and sort them in descending order\n",
    "correlation_values = corr_mat.abs().unstack().sort_values(ascending=False)\n",
    "\n",
    "# Remove self-correlation (correlation of a feature with itself)\n",
    "correlation_values = correlation_values[correlation_values.index.get_level_values(0) != correlation_values.index.get_level_values(1)]\n",
    "\n",
    "# Select the top correlated features based on the threshold\n",
    "top_correlated = correlation_values[correlation_values >= threshold]\n",
    "\n",
    "# Print the top correlated features and their correlation coefficient\n",
    "\"\"\"for (feature1, feature2), correlation in top_correlated.items():\n",
    "    print(f\"Features: {feature1} and {feature2}, Correlation Coefficient: {correlation}\")\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"pd.set_option('display.max_rows', None)\n",
    "print(none_data)\n",
    "pd.set_option('display.max_rows', 60)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6c2af236-7381-4fea-bb4a-4c363f6d0634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropCorr(data, threshold = .9):\n",
    "    corr_df = data.corr().abs()\n",
    "    mask = np.triu(np.ones_like(corr_df, dtype = bool))\n",
    "    tri_df = corr_df.mask(mask)\n",
    "    to_drop = [c for c in tri_df.columns if any(tri_df[c] > threshold)]\n",
    "    return to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "87809649-901b-4692-b66a-9023ae6dee29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleData(data):\n",
    "# Step 1: Initialize the StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Step 2: Fit the scaler on\n",
    "    # your data (estimate mean and standard deviation)\n",
    "    scaler.fit(data)\n",
    "    \n",
    "    # Step 3: Transform the data to standardized scale\n",
    "    data_scaled = scaler.transform(data)\n",
    "    data_scaled = pd.DataFrame(data_scaled, columns = data.columns)\n",
    "    data = data_scaled\n",
    "    return data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "425eb7a9-8a14-4c20-b538-d0e8a90c63f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runXGBModel(X, Y, featRank = False, plot = False, n_features = 10):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "    feature_names = X.columns\n",
    "    \n",
    "    model = xgb.XGBRegressor()\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    feature_importances = model.feature_importances_\n",
    "    indices = np.argsort(feature_importances)[::-1]\n",
    "\n",
    "    y_pred = model.predict(x_test)\n",
    "    print(\"XGBoost MSE: \", mean_squared_error(y_test,y_pred))\n",
    "\n",
    "    if featRank:\n",
    "        print(\"XGBoost Feature ranking:\")\n",
    "        for i, index in enumerate(indices):\n",
    "            print(f\"{i + 1}. Feature {feature_names[index]}: {feature_importances[index]}\")\n",
    "            if i > n_features:\n",
    "                break\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    if plot:\n",
    "        # Plot feature importances\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.title(\"XGBoost Feature Importances\")\n",
    "        plt.bar(range(n_features), feature_importances[indices][:n_features], align=\"center\")\n",
    "        plt.xticks(range(n_features), [feature_names[indices[i]] for i in range(n_features)], rotation=45)\n",
    "        plt.xlabel(\"Feature Index\")\n",
    "        plt.ylabel(\"Importance Score\")\n",
    "        plt.show()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "300fbf9d-535a-44a5-8815-83132c888f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the fitted Lasso model\n",
    "def runLassoModel(X, Y):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "    # Instantiate the Lasso model\n",
    "    lasso = Lasso(alpha=0.1,max_iter=10000)\n",
    "    \n",
    "    # Fit the model to the training data\n",
    "    lasso.fit(x_train, y_train)\n",
    "    \n",
    "    # Perform cross-validation on the training data\n",
    "    cv_scores = cross_val_score(lasso, x_train, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "    \n",
    "    # Convert scores to positive values and take the mean\n",
    "    cv_scores = -cv_scores  # Convert to positive values\n",
    "    avg_cv_score = cv_scores.mean()\n",
    "    \n",
    "    print(\"Lasso Cross-validated MSE:\", avg_cv_score)\n",
    "    \n",
    "    # Optionally, evaluate the model on the testing data\n",
    "    test_score = lasso.score(x_test, y_test)\n",
    "    print(\"Lasso R^2 Score:\", test_score)\n",
    "    return lasso\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b29afe5b-0736-4c00-aab2-881875f5d197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the fitted Lasso model\n",
    "def runRidgeModel(X, Y):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "    # Instantiate the Lasso model\n",
    "    ridge = Ridge(alpha=0.1,max_iter=10000)\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    ridge.fit(x_train, y_train)\n",
    "    \n",
    "    # Perform cross-validation on the training data\n",
    "    cv_scores = cross_val_score(ridge, x_train, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "    \n",
    "    # Convert scores to positive values and take the mean\n",
    "    cv_scores = -cv_scores  # Convert to positive values\n",
    "    avg_cv_score = cv_scores.mean()\n",
    "    \n",
    "    print(\"Ridge Cross-validated MSE:\", avg_cv_score)\n",
    "    \n",
    "    # Optionally, evaluate the model on the testing data\n",
    "    test_score = ridge.score(x_test, y_test)\n",
    "    print(\"Ridge R^2 Score:\", test_score)\n",
    "    return ridge\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e56b8726-afb7-45f0-8ec7-a9f0472de1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runPCA(X, n_components = 2):\n",
    "    pca = PCA(n_components)\n",
    "    \n",
    "    pca.fit(X)\n",
    "\n",
    "    return pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "96cd8f40-5210-437c-905e-a4b3a39208b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runRF(X, Y, n_estimators = 100, featRank = False, plot = False, n_features = 10):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "    \n",
    "    rf = RandomForestRegressor(n_estimators = n_estimators)\n",
    "    rf.fit(x_train, y_train)\n",
    "    \n",
    "    y_pred = rf.predict(x_test)\n",
    "    print(\"Random Forest MSE: \", mean_squared_error(y_test, y_pred))\n",
    "\n",
    "    feature_importances = rf.feature_importances_\n",
    "    feature_names = X.columns\n",
    "    indices = np.argsort(feature_importances)[::-1]\n",
    "    \n",
    "    if featRank:\n",
    "        print(\"Random Forest Feature ranking:\")\n",
    "        for i, index in enumerate(indices):\n",
    "            print(f\"{i + 1}. Feature {feature_names[index]}: {feature_importances[index]}\")\n",
    "            if i >= n_features-1:\n",
    "                break\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    if plot:\n",
    "        feature_importances = rf.feature_importances_\n",
    "        sorted_indices = feature_importances.argsort()[::-1]\n",
    "        # Plot the feature importances\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.title(\"Random Forest Feature Importances\")\n",
    "        plt.bar(range(n_features), feature_importances[indices][:n_features], align=\"center\")\n",
    "        plt.xticks(range(n_features), [feature_names[indices[i]] for i in range(n_features)], rotation=90)\n",
    "        plt.xlabel(\"Feature\")\n",
    "        plt.ylabel(\"Importance\")\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    return rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ba93bdbc-2e13-422a-83be-265ff9dd75df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runRegression(X, Y):\n",
    "    # Split the data into training and testing sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "    \n",
    "    # Initialize and fit the model\n",
    "    model = LinearRegression()\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    predictions = model.predict(x_test)\n",
    "    \n",
    "    # Calculate MSE and R-squared\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    r_squared = r2_score(y_test, predictions)\n",
    "    \n",
    "    return model, predictions, y_test, mse, r_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7dac74b6-2cfc-49e8-a0bc-b6dc1c74d6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "def runCNN(X, Y, plot = False, epochs = 10, batch_size = 32, learning_rate = .001):\n",
    "\n",
    "    # splits the temporary data into training and validation sets\n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "    accuracies = []\n",
    "    losses = []\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(X,Y, test_size=0.2)    \n",
    "    \n",
    "    for train_index, val_index in kf.split(x_train):\n",
    "\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        x_train_fold, x_val_fold = x_train.iloc[train_index], x_train.iloc[val_index]\n",
    "\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "        \n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(64, activation='relu', input_shape=(X.shape[1],)),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "        \n",
    "        # Compile the model\n",
    "        model.compile(optimizer=optimizer, \n",
    "                      loss='huber_loss',\n",
    "                      metrics=['mse'])\n",
    "    \n",
    "        history = model.fit(x_train_fold, y_train_fold, epochs = epochs, batch_size=batch_size, validation_data=(x_val_fold, y_val_fold))\n",
    "        \n",
    "        # Evaluate the model on the validation set\n",
    "        loss, accuracy = model.evaluate(x_val_fold, y_val_fold)\n",
    "        accuracies.append(accuracy)\n",
    "        losses.append(loss)\n",
    "\n",
    "\n",
    "    \n",
    "    avg_accuracy = np.mean(accuracies)\n",
    "    avg_loss = np.mean(losses)\n",
    "\n",
    "    print(f'Avg Accuracy: {avg_accuracy}')\n",
    "    print(f'Avg Loss: {avg_loss}')\n",
    "    \n",
    "    if plot:\n",
    "        # Plot training & validation loss values\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('Model loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot training & validation accuracy values\n",
    "        plt.plot(history.history['mse'])\n",
    "        plt.plot(history.history['val_mse'])\n",
    "        plt.title('Model accuracy')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "        plt.show()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "623ad7aa-33a0-404b-90dd-468c209e8426",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m x_train, x_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(x,y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m cnn_model \u001b[38;5;241m=\u001b[39m runCNN(x,y,plot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m      4\u001b[0m loss, mse \u001b[38;5;241m=\u001b[39m cnn_model\u001b[38;5;241m.\u001b[39mevaluate(x_test, y_test)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[85], line 17\u001b[0m, in \u001b[0;36mrunCNN\u001b[1;34m(X, Y, plot, epochs, batch_size, learning_rate)\u001b[0m\n\u001b[0;32m     13\u001b[0m y_train_fold, y_val_fold \u001b[38;5;241m=\u001b[39m y_train\u001b[38;5;241m.\u001b[39miloc[train_index], y_train\u001b[38;5;241m.\u001b[39miloc[val_index]\n\u001b[0;32m     14\u001b[0m x_train_fold, x_val_fold \u001b[38;5;241m=\u001b[39m x_train\u001b[38;5;241m.\u001b[39miloc[train_index], x_train\u001b[38;5;241m.\u001b[39miloc[val_index]\n\u001b[1;32m---> 17\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate \u001b[38;5;241m=\u001b[39m learning_rate)\n\u001b[0;32m     19\u001b[0m model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[0;32m     20\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m64\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],)),\n\u001b[0;32m     21\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m64\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     22\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     23\u001b[0m ])\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2)\n",
    "\n",
    "cnn_model = runCNN(x,y,plot = True, epochs = 10, batch_size = 32)\n",
    "loss, mse = cnn_model.evaluate(x_test, y_test)\n",
    "print(f\"Loss: {loss}\\nMSE: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee2d7fc-9a12-45d8-bfcb-1e90e3ff7ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
